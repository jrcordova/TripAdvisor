{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reqcuired Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asign variables to urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url1 = The review Page of attraction #1\n",
    "url2 = The review Page of attraction #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important!\n",
    "It is essential to use the Review page as the URL and not the attraction page. Otherwise, reviews only display a portion of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of URls\n",
    "WebSites = [url1, url2, url3, etc...]\n",
    "exten = '.csv'\n",
    "File = r'Your File Here'\n",
    "#Progressor\n",
    "q = 1\n",
    "#Loop through each URL\n",
    "for theurl in WebSites:\n",
    "    #Create empty lists that will feed the pandas data frame\n",
    "    location = []\n",
    "    date = []\n",
    "    rating = []\n",
    "    texto = []\n",
    "    atractions = []\n",
    "    contribution = []\n",
    "    # Title of output csv\n",
    "    t = theurl.split('-')[4:]\n",
    "    tit = ''.join(t)\n",
    "    title = tit.strip('.html#REVIEWS')\n",
    "    df = pd.DataFrame()\n",
    "    # Get Soup\n",
    "    thepage = urllib.request.urlopen(theurl)\n",
    "    sopa = bs.BeautifulSoup(thepage, \"html.parser\")\n",
    "    while True:\n",
    "        my_div = sopa.find_all(\"div\", class_=\"reviewSelector\")\n",
    "        # Loop through each review window\n",
    "        for div in my_div:\n",
    "            user = div.find_all(\"div\", class_ = \"username\")\n",
    "            for use in user:\n",
    "                nuser = str(use.text)\n",
    "                # If user dosent have personal information, skip (It is troublesome to deal with these users)\n",
    "                if nuser.strip() != \"A TripAdvisor Member\":\n",
    "                    # Acquire Country of origin of user\n",
    "                    loc = div.find_all(\"div\", class_=\"location\")\n",
    "                    for l in loc:\n",
    "                        location.append(l.string)\n",
    "                    span = div.find_all(\"div\", class_='rating reviewItemInline')\n",
    "                    # Acquire Date of review\n",
    "                    for s in span:\n",
    "                        if div.find(attrs={'class': 'ratingDate relativeDate'}) in s:\n",
    "                            pre = div.find_all(attrs={'class':\"ratingDate relativeDate\"})  \n",
    "                            for pr in pre:\n",
    "                                date.append(pr.attrs['title'])            \n",
    "                        else:  \n",
    "                            al = div.find_all(attrs={'class':'ratingDate'})\n",
    "                            for ka in al:\n",
    "                                date.append(ka.string)   \n",
    "                    # Acquire text of Review            \n",
    "                    tex = div.find_all(\"div\", class_= \"entry\")\n",
    "                    for t in tex:\n",
    "                        texto.append(t.text)    \n",
    "                    toFind = re.compile(\"(bubble_[0-9])+\")\n",
    "                    for s in div.findAll(\"span\", {\"class\":toFind}):\n",
    "                        ran = s\n",
    "                        rating.append(ran)\n",
    "                    # Acquire number of reviews posted by user and how helpfull they are   \n",
    "                    badge = div.find_all(\"div\", class_= \"memberBadging g10n\")\n",
    "                    for b in badge:\n",
    "                        if div.find(\"div\", class_= \"helpfulVotesBadge badge no_cpu\") in b:\n",
    "                            helpf = div.find_all(\"div\", class_= \"helpfulVotesBadge badge no_cpu\")\n",
    "                            for h in helpf:\n",
    "                                contribution.append(h.text)\n",
    "                        else:\n",
    "                            contribution.append(\"0 helpful votes\")    \n",
    "                    att = div.find_all('div', class_= \"reviewerBadge badge\")\n",
    "                    for a in att:\n",
    "                        atractions.append(a.text)\n",
    "                else: pass         \n",
    "                \n",
    "        link = sopa.find_all(attrs={\"class\": \"nav next rndBtn ui_button primary taLnk\"})\n",
    "        if len(link) == 0:\n",
    "            # Clean up data to make it more usable\n",
    "            country = []\n",
    "            for item in location:\n",
    "                country.append(item.split(\",\")[-1])\n",
    "            pais = []      \n",
    "            for it in country:\n",
    "                pais.append(it.strip(\"\\n\"))    \n",
    "            text = []\n",
    "            for rev in texto:\n",
    "                t = rev.strip(\"\\n\")\n",
    "                ts = str(t)\n",
    "                text.append(ts)\n",
    "                helpful_votes = []\n",
    "            for votes in contribution:\n",
    "                streper = votes.strip(\"\\n\")\n",
    "                helpful_votes.append(streper.split()[0])\n",
    "                reviewed_at = []\n",
    "            for rev in atractions:\n",
    "                reveti = rev.strip(\"\\n\")\n",
    "                reviewed_at.append(reveti.split()[0])\n",
    "            # Convert image name of rating into a value from 1 to 5    \n",
    "            Stars = []\n",
    "            for item in rating:\n",
    "                x = str(item)\n",
    "                if x.endswith('bubble_1\"></span>') == True:\n",
    "                    Stars.append(1)\n",
    "                elif x.endswith('bubble_2\"></span>') == True:\n",
    "                    Stars.append(2)\n",
    "                elif x.endswith('bubble_3\"></span>') == True:\n",
    "                    Stars.append(3)\n",
    "                elif x.endswith('bubble_4\"></span>') == True:\n",
    "                    Stars.append(4)\n",
    "                else:\n",
    "                    Stars.append(5)  \n",
    "            # Insert Data into Pandas Data Frame        \n",
    "            df[\"Date\"] = date\n",
    "            df[\"Country of Origin\"] = pais\n",
    "            df[\"Review\"] = text\n",
    "            df[\"Rating\"] = Stars\n",
    "            df[\"reviewed attractions\"] = reviewed_at\n",
    "            df[\"helpful votes\"] = helpful_votes \n",
    "            new = []\n",
    "            for item in df['Date']:\n",
    "                x = item.strip('Reviewed')\n",
    "                y = x.strip('\\n')\n",
    "                new.append(y)\n",
    "            df.drop('Date', axis = 1)\n",
    "            # Convert Date into Month and Year\n",
    "            df['Date'] = new\n",
    "            df['Month'] = df['Date'].apply(lambda x: x.split()[0])\n",
    "            df['Year'] = df['Date'].apply(lambda x: x.split()[-1])\n",
    "            # Create path of csv file\n",
    "            path = os.path.join(File, title)\n",
    "            df.to_csv(path + exten)\n",
    "            # Progressor label\n",
    "            print(\"Page {} out of {}\".format(q, len(WebSites)))\n",
    "            q = q+1\n",
    "            break\n",
    "        # Feed next Review Page    \n",
    "        else:\n",
    "            sopa = bs.BeautifulSoup(urllib.request.urlopen(\"http://www.tripadvisor.com\" + link[0].get('href')),\"html.parser\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
